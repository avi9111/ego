{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from boto import s3\n",
    "\n",
    "false_channel = [\n",
    "        \"130134201\",\n",
    "        \"110134101106.0\",\n",
    "        \"13013412700\",\n",
    "        \"13013419400\",\n",
    "        \"130134164\",\n",
    "        \"47\",\n",
    "        \"13013415800\",\n",
    "        \"1301343257\",\n",
    "        \"13013413400\",\n",
    "        \"131\",\n",
    "        \"142\",\n",
    "        \"145\",\n",
    "        \"180\",\n",
    "        \"188\",\n",
    "        \"196\",\n",
    "        \"204\",\n",
    "        \"210\",\n",
    "        \"213\",\n",
    "        \"215\",\n",
    "        \"216\",\n",
    "        \"223\",\n",
    "        \"234\",\n",
    "        \"250\",\n",
    "        \"259\",\n",
    "        \"262\",\n",
    "        \"263\",\n",
    "        \"265\",\n",
    "        \"292\",\n",
    "        \"296\",\n",
    "        \"298\",\n",
    "        \"306\",\n",
    "        \"309\",\n",
    "        \"313\",\n",
    "        \"1301341500\",\n",
    "        \"1301341232\",\n",
    "        \"13013416000\",\n",
    "        \"13013411000\",\n",
    "        \"1301344600\",\n",
    "        \"1301345600\",\n",
    "        \"13013412000\",\n",
    "        \"1301341272\",\n",
    "        \"13013415900\",\n",
    "        \"130134137\",\n",
    "        \"1301371335\",\n",
    "        \"130134126\",\n",
    "        \"1301343900\",\n",
    "        \"13013415200\",\n",
    "        \"13013423200\",\n",
    "        \"1301343234\",\n",
    "        \"130134312108\",\n",
    "        \"1301341300\",\n",
    "        \"130134312173\",\n",
    "        \"5000\",\n",
    "        \"130134159\",\n",
    "        \"1301343800\",\n",
    "        \"1301345400\",\n",
    "        \"1301341\",\n",
    "        \"130134144\",\n",
    "        \"13013443300\",\n",
    "        \"268\",\n",
    "        \"350\",\n",
    "        \"389\"\n",
    "        ]\n",
    "true_channel =[\n",
    "        \"130134000201\",\n",
    "        \"110134101106\",\n",
    "        \"130134012700\",\n",
    "        \"130134019400\",\n",
    "        \"130134000164\",\n",
    "        \"47\",\n",
    "        \"130134015800\",\n",
    "        \"130134003257\",\n",
    "        \"130134013400\",\n",
    "        \"131\",\n",
    "        \"142\",\n",
    "        \"145\",\n",
    "        \"180\",\n",
    "        \"188\",\n",
    "        \"196\",\n",
    "        \"204\",\n",
    "        \"210\",\n",
    "        \"213\",\n",
    "        \"215\",\n",
    "        \"216\",\n",
    "        \"223\",\n",
    "        \"234\",\n",
    "        \"250\",\n",
    "        \"259\",\n",
    "        \"262\",\n",
    "        \"263\",\n",
    "        \"265\",\n",
    "        \"292\",\n",
    "        \"296\",\n",
    "        \"298\",\n",
    "        \"306\",\n",
    "        \"309\",\n",
    "        \"313\",\n",
    "        \"130134001500\",\n",
    "        \"130134001232\",\n",
    "        \"130134016000\",\n",
    "        \"130134011000\",\n",
    "        \"130134004600\",\n",
    "        \"130134005600\",\n",
    "        \"130134012000\",\n",
    "        \"130134001272\",\n",
    "        \"130134015900\",\n",
    "        \"130134000137\",\n",
    "        \"130134001335\",\n",
    "        \"130134000126\",\n",
    "        \"130134003900\",\n",
    "        \"130134015200\",\n",
    "        \"130134023200\",\n",
    "        \"130134003234\",\n",
    "        \"130134312108\",\n",
    "        \"130134001300\",\n",
    "        \"130134312173\",\n",
    "        \"5000\",\n",
    "        \"130134000159\",\n",
    "        \"130134003800\",\n",
    "        \"130134005400\",\n",
    "        \"130134000001\",\n",
    "        \"130134000144\",\n",
    "        \"130134043300\",\n",
    "        \"268\",\n",
    "        \"350\",\n",
    "        \"389\"\n",
    "        ]\n",
    "\n",
    "properties = {\n",
    "    \"user\": \"test1\",\n",
    "    \"password\": \"QmPhaQ8hYsxx\"\n",
    "}\n",
    "tableName = \"uid_creatTime\"\n",
    "url = \"jdbc:mysql://54.223.192.252:3306/test\"\n",
    "\n",
    "def get_s3_list(daterange):\n",
    "    prefix = \"logics3/psid=4\"\n",
    "    total_size = 0\n",
    "    REGION = \"cn-north-1\"\n",
    "    conn = s3.connect_to_region(REGION)\n",
    "    bucket = conn.lookup('prodlog')\n",
    "    ret = []\n",
    "    if bucket:\n",
    "        for k in bucket.list(prefix=prefix):\n",
    "            if k.size <=0:\n",
    "                continue\n",
    "            logsp = k.name.split('.')\n",
    "            a = logsp[-4:-1]\n",
    "            a.reverse()\n",
    "            dt = ''.join(a)\n",
    "            if dt == daterange:\n",
    "                total_size += k.size\n",
    "                ret.append('s3://prodlog/'+k.name)\n",
    "                print('s3://prodlog/'+k.name, ''.join(a))\n",
    "    print('total:%d'%(total_size/1024.0/1024.0/1024.0))\n",
    "    return ret\n",
    "\n",
    "def min_time(x,y):\n",
    "    if int(x.lt) > int(y.lt):\n",
    "        return y\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    begin = datetime.date(2016,12,1)\n",
    "    end = datetime.date(2017,5,12)\n",
    "\n",
    "    for i in range((end - begin).days + 1):\n",
    "        xx = str(begin + datetime.timedelta(days=i))\n",
    "        pp = xx.split('-')\n",
    "        day = \"\".join(pp)\n",
    "\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "            pass\n",
    "        spark = SparkSession \\\n",
    "            .builder.enableHiveSupport() \\\n",
    "            .appName(\"Python Spark SQL basic example\") \\\n",
    "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "\n",
    "        df = spark.read.json(get_s3_list(day))\n",
    "\n",
    "        df.createOrReplaceTempView('RawTable')\n",
    "        preparetable = 'RawTable'\n",
    "\n",
    "        result1 = spark.sql(u'''\n",
    "        SELECT\n",
    "            *\n",
    "        FROM %s\n",
    "        WHERE accountid is not null \n",
    "        AND type_name!=\"CorpLevelChg\" \n",
    "        '''%preparetable).cache().createOrReplaceTempView('jsonTable')\n",
    "\n",
    "        #获取当日log里的最早的一次登录\n",
    "        spark.sql(u\"\"\"\n",
    "        SELECT\n",
    "            t.uid AS uid,\n",
    "            SUBSTRING(t.lt,0,10) AS lt,\n",
    "            CAST(jsonTable.utc8 as DATE) AS date,\n",
    "            jsonTable.sid AS sid,\n",
    "            jsonTable.gid AS gid,\n",
    "            jsonTable.channel AS ch,\n",
    "            jsonTable.info.MachineType AS machine\n",
    "        FROM (\n",
    "            SELECT\n",
    "                userid AS uid,\n",
    "                min(logtime) AS lt\n",
    "            FROM jsonTable\n",
    "            WHERE type_name=\"Login\"\n",
    "            GROUP BY userid\n",
    "        ) t\n",
    "        LEFT JOIN jsonTable ON jsonTable.userid = t.uid AND jsonTable.logtime = t.lt\n",
    "        \"\"\").cache().createOrReplaceTempView('account_ch')\n",
    "        \n",
    "        #获取当日log里分服务器玩家角色的第一次登录\n",
    "        spark.sql(u\"\"\"\n",
    "        SELECT\n",
    "            t.acid AS acid,\n",
    "            SUBSTRING(t.lt,0,10) AS lt,\n",
    "            CAST(jsonTable.utc8 as DATE) AS date,\n",
    "            jsonTable.sid AS sid,\n",
    "            jsonTable.gid AS gid,\n",
    "            jsonTable.channel AS ch,\n",
    "            jsonTable.info.MachineType AS machine\n",
    "        FROM (\n",
    "            SELECT\n",
    "                accountid AS acid,\n",
    "                min(logtime) AS lt\n",
    "            FROM jsonTable\n",
    "            WHERE type_name=\"Login\"\n",
    "            GROUP BY accountid\n",
    "        ) t\n",
    "        LEFT JOIN jsonTable ON jsonTable.accountid = t.acid AND jsonTable.logtime = t.lt\n",
    "        \"\"\").cache().createOrReplaceTempView('accountID_ch')\n",
    "        \n",
    "        #\n",
    "        newlogiclog_df = spark.sql(u\"\"\"\n",
    "        SELECT * \n",
    "        FROM account_ch \n",
    "        \"\"\")\n",
    "        \n",
    "        newlogiclog_acid_df = spark.sql(u\"\"\"\n",
    "        SELECT * \n",
    "        FROM accountID_ch \n",
    "        \"\"\")\n",
    "\n",
    "        #替换channel\n",
    "        newlogiclog = newlogiclog_df.replace(false_channel,true_channel,\"ch\")\n",
    "        newlogiclog_acid = newlogiclog_acid_df.replace(false_channel,true_channel,\"ch\")\n",
    "        #读取mysql\n",
    "        dataframe_mysql = spark.read.format(\"jdbc\").option(\"url\", url).option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", tableName).option(\"user\", properties[user]).option(\"password\", properties[password]).load()\n",
    "        dataframe_mysql_acid = spark.read.format(\"jdbc\").option(\"url\", url).option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"account_creatTime\").option(\"user\", properties[user]).option(\"password\", properties[password]).load()\n",
    "        #写mysql\n",
    "        finalResult = dataframe_mysql.rdd.union(newlogiclog).map(lambda x: (x.uid,x)).reduceByKey(min_time).values()\n",
    "        spark.createDataFrame(finalResult).write.jdbc(url, tableName,\"append\", properties)\n",
    "        \n",
    "        final_acid_Result = dataframe_mysql.rdd.union(newlogiclog_acid).map(lambda x: (x.uid,x)).reduceByKey(min_time).values()\n",
    "        spark.createDataFrame(final_acid_Result).write.jdbc(url, \"account_creatTime\",\"append\", properties)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
