{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-28 18:39:12 creat spark session\n",
      "2017-06-28 18:39:12 start: 20170201\n",
      "2017-06-28 18:39:12 finish read original data\n",
      "2017-06-28 18:39:12 finish creat today jsonTable\n",
      "2017-06-28 18:39:12 finish IAP\n",
      "2017-06-28 18:39:12 finish uid Login\n",
      "2017-06-28 18:39:12 finish acid Login\n",
      "2017-06-28 18:39:12 finish all replace\n",
      "uid Creat Time:\n",
      "+--------------------+----------+----------+----+---+------------+--------------------+\n",
      "|                 uid|        lt|      date| sid|gid|          ch|             machine|\n",
      "+--------------------+----------+----------+----+---+------------+--------------------+\n",
      "|38420ce1-b171-4ee...|1485909512|2017-02-01|1001|200|130134005400|         Xiaomi MI 5|\n",
      "|b4c12f87-3751-4c3...|1485913264|2017-02-01|1001|200|130134004600|YuLong Coolpad 86...|\n",
      "|504717e5-384b-466...|1485919808|2017-02-01|1001|200|130134001500|         Meizu PRO 5|\n",
      "+--------------------+----------+----------+----+---+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "acid Creat Time:\n",
      "+--------------------+----------+----------+----+---+\n",
      "|                acid|        lt|      date| sid|gid|\n",
      "+--------------------+----------+----------+----+---+\n",
      "|200:1001:006c83d5...|1485915425|2017-02-01|1001|200|\n",
      "|200:1001:8dea1830...|1485928256|2017-02-01|1001|200|\n",
      "|200:1001:e9fa42f7...|1485958029|2017-02-01|1001|200|\n",
      "+--------------------+----------+----------+----+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+----------+----------+----+---+\n",
      "|                 uid|        lt|      date| sid|gid|\n",
      "+--------------------+----------+----------+----+---+\n",
      "|697b6f94-2f7a-4e2...|1485946542|2017-02-01|1001|200|\n",
      "|e842a215-2a8b-422...|1485966165|2017-02-02|1001|200|\n",
      "|a6e03dd8-3851-405...|1485926135|2017-02-01|1001|200|\n",
      "+--------------------+----------+----------+----+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "payLog:\n",
      "+--------------------+--------------------+---+----+------------+-------+------+------+--------------+----------+----------+\n",
      "|                 uid|                acid|gid| sid|          ch|corpLvl|vipLvl|payNum|       payTime|   LogTime|      date|\n",
      "+--------------------+--------------------+---+----+------------+-------+------+------+--------------+----------+----------+\n",
      "|7a576463-90f9-420...|200:1001:7a576463...|200|1001|130134001300|     81|    11|   600|20170201075826|1485907229|2017-02-01|\n",
      "|fcf736a7-abf4-44e...|200:1001:fcf736a7...|200|1001|        5000|     69|     6|   100|20170201081234|1485907988|2017-02-01|\n",
      "|fcf736a7-abf4-44e...|200:1001:fcf736a7...|200|1001|        5000|     69|     6|  3000|20170201081332|1485908044|2017-02-01|\n",
      "+--------------------+--------------------+---+----+------------+-------+------+------+--------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2017-06-28 18:39:14 finish today data\n",
      "2017-06-28 18:39:14 union today uid_creatTime\n",
      "[]\n",
      "2017-06-28 18:39:14 union today uid_creatTime\n",
      "[]\n",
      "2017-06-28 18:39:14 union today acid_creatTime \n",
      "[]\n",
      "2017-06-28 18:39:15 union today uid_ByChannelcreatTime \n",
      "[]\n",
      "2017-06-28 18:39:15 start ltv\n",
      "2017-06-28 18:39:15 finish today data\n",
      "2017-06-28 18:39:15 start: 20170202\n",
      "2017-06-28 18:39:15 finish read original data\n",
      "2017-06-28 18:39:15 finish creat today jsonTable\n",
      "2017-06-28 18:39:15 finish IAP\n",
      "2017-06-28 18:39:15 finish uid Login\n",
      "2017-06-28 18:39:15 finish acid Login\n",
      "2017-06-28 18:39:15 finish all replace\n",
      "uid Creat Time:\n",
      "+--------------------+----------+----------+----+---+------------+--------------------+\n",
      "|                 uid|        lt|      date| sid|gid|          ch|             machine|\n",
      "+--------------------+----------+----------+----+---+------------+--------------------+\n",
      "|504717e5-384b-466...|1485995478|2017-02-02|1001|200|130134001500|         Meizu PRO 5|\n",
      "|38420ce1-b171-4ee...|1486007578|2017-02-02|1001|200|130134005400|         Xiaomi MI 5|\n",
      "|c9b57475-3816-44a...|1486009039|2017-02-02|1001|200|130134001500|HUAWEI HUAWEI NXT...|\n",
      "+--------------------+----------+----------+----+---+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "acid Creat Time:\n",
      "+--------------------+----------+----------+----+---+\n",
      "|                acid|        lt|      date| sid|gid|\n",
      "+--------------------+----------+----------+----+---+\n",
      "|200:1001:8056ce7d...|1486046911|2017-02-02|1001|200|\n",
      "|200:1001:c5181815...|1486002557|2017-02-02|1001|200|\n",
      "|200:1001:10ee9747...|1486021301|2017-02-02|1001|200|\n",
      "+--------------------+----------+----------+----+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+----------+----------+----+---+\n",
      "|                 uid|        lt|      date| sid|gid|\n",
      "+--------------------+----------+----------+----+---+\n",
      "|471ca065-e854-4d5...|1486000886|2017-02-02|1001|200|\n",
      "|36322af9-7603-4a8...|1486042246|2017-02-02|1001|200|\n",
      "|7fce27d1-7e3e-40e...|1485999708|2017-02-02|1001|200|\n",
      "+--------------------+----------+----------+----+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "payLog:\n",
      "+--------------------+--------------------+---+----+------------+-------+------+------+--------------+----------+----------+\n",
      "|                 uid|                acid|gid| sid|          ch|corpLvl|vipLvl|payNum|       payTime|   LogTime|      date|\n",
      "+--------------------+--------------------+---+----+------------+-------+------+------+--------------+----------+----------+\n",
      "|f73ef3ed-6acc-4bc...|200:1001:f73ef3ed...|200|1001|130134001300|     79|    11|   600|20170202080100|1485993691|2017-02-02|\n",
      "|c920b03e-8011-494...|200:1001:c920b03e...|200|1001|130134001500|     30|     0|   600|20170202080324|1485993985|2017-02-02|\n",
      "|e4b5d937-76ad-487...|200:1001:e4b5d937...|200|1001|130134001300|     72|     9|  2500|20170202080933|1485994227|2017-02-02|\n",
      "+--------------------+--------------------+---+----+------------+-------+------+------+--------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2017-06-28 18:39:17 finish today data\n",
      "2017-06-28 18:39:17 union today uid_creatTime\n",
      "['/Users/tq/bigdatatest/uid_creatTime.json']\n",
      "2017-06-28 18:39:20 union today uid_creatTime\n",
      "['/Users/tq/bigdatatest/acid_creatTime.json']\n",
      "acidListMerge                                               acid  gid   sid        date  \\\n",
      "0    200:1001:9ad0faf7-feac-4d22-b750-aab9fdd9d6d1  200  1001  2017-02-02   \n",
      "1    200:1001:cdf8010a-e27e-4407-94d1-c050067fa578  200  1001  2017-02-01   \n",
      "2    200:1001:5c4d7820-e97d-4ab6-b194-2f2fcd6c425d  200  1001  2017-02-02   \n",
      "3    200:1001:d9fde481-e002-43a0-8f04-f25276832171  200  1001  2017-02-02   \n",
      "4    200:1001:ac0c133b-6749-4add-b381-fb19aabd64da  200  1001  2017-02-02   \n",
      "5    200:1001:3f1dc78e-b5c2-48fd-9b57-5ec99e973148  200  1001  2017-02-02   \n",
      "6    200:1001:20db349a-3e2f-45c9-89ae-bfd1f93ea5d6  200  1001  2017-02-02   \n",
      "7    200:1001:3f20c3a9-fa40-4cf7-b152-8c0224081e49  200  1001  2017-02-02   \n",
      "8    200:1001:beb74c9d-808b-40a9-a0f9-c9bcf1c88caf  200  1001  2017-02-01   \n",
      "9    200:1001:b937422c-217f-4a19-aaf5-06abdf860374  200  1001  2017-02-02   \n",
      "10   200:1001:f57301fc-07da-47fb-a983-54246ad0fea6  200  1001  2017-02-02   \n",
      "11   200:1001:0affc2c9-6eaa-46aa-9faa-0fc01c132970  200  1001  2017-02-01   \n",
      "12   200:1001:8056ce7d-86dd-4e7c-847c-370f2d742232  200  1001  2017-02-02   \n",
      "13   200:1001:73b6d305-baea-4aed-bd1a-036b37c63c0f  200  1001  2017-02-01   \n",
      "14   200:1001:fc647982-e056-4787-a1b5-b05c17d3add9  200  1001  2017-02-01   \n",
      "15   200:1001:ea7a0a7d-4756-4fe0-a1c3-2d9300004df2  200  1001  2017-02-01   \n",
      "16   200:1001:9156b312-d191-46cf-bc4c-f8177307f2c3  200  1001  2017-02-02   \n",
      "17   200:1001:0f6945ae-c903-43cd-bcaa-6884487efc60  200  1001  2017-02-01   \n",
      "18   200:1001:b4aace06-4e67-4666-b1c5-f3695359567b  200  1001  2017-02-02   \n",
      "19   200:1001:326730d8-7eee-4244-85da-5c1b2404b215  200  1001  2017-02-02   \n",
      "20   200:1001:a7e7412d-864c-43ae-8183-8d4beba7ad3c  200  1001  2017-02-02   \n",
      "21   200:1001:3b18975a-6e7c-4d7b-9cd5-f726df8cac65  200  1001  2017-02-01   \n",
      "22   200:1001:82c80635-c99b-4834-9ea7-9bf7995ca2f2  200  1001  2017-02-02   \n",
      "23   200:1001:bc649ab8-d4aa-4ae4-b32f-71ee9dc88086  200  1001  2017-02-02   \n",
      "24   200:1001:1c2a984c-1201-4ba4-a35e-f2d3dbe520e4  200  1001  2017-02-02   \n",
      "25   200:1001:8343d30f-b767-4f2c-be40-cc4be52eed78  200  1001  2017-02-02   \n",
      "26   200:1001:0b73dff2-f6b1-4d79-ab9a-05a28148a2a1  200  1001  2017-02-02   \n",
      "27   200:1001:dc69b490-fd3a-49db-88a7-1b24777ec6b3  200  1001  2017-02-02   \n",
      "28   200:1001:10ee9747-d18f-4c68-b286-00719aca6f3c  200  1001  2017-02-02   \n",
      "29   200:1001:d3ca056e-726a-4738-961a-3737bb41aa44  200  1001  2017-02-02   \n",
      "..                                             ...  ...   ...         ...   \n",
      "232  200:1001:41a36401-6ea2-4f71-8482-e11b94700406  200  1001  2017-02-02   \n",
      "233  200:1001:7e112ab1-03a2-4a09-851b-959e7d4ed779  200  1001  2017-02-01   \n",
      "234  200:1001:36ee7eb7-3341-4014-8952-38257fbd6686  200  1001  2017-02-02   \n",
      "235  200:1001:8033eba2-7634-4e43-8851-fbaf2d41a04e  200  1001  2017-02-01   \n",
      "236  200:1001:7b0bccec-1104-43ed-863b-8db301ad13ab  200  1001  2017-02-01   \n",
      "237  200:1001:15d0910d-ceac-47bc-b8d7-c9c187c54220  200  1001  2017-02-02   \n",
      "238  200:1001:d3dca8da-78bc-4135-8f40-84f4b4e62465  200  1001  2017-02-02   \n",
      "239  200:1001:3d1395f2-fedc-4a44-b3e8-3fd6fba88ff8  200  1001  2017-02-01   \n",
      "240  200:1001:9d8e1751-26e5-4793-99cd-cf49b0a88b7e  200  1001  2017-02-01   \n",
      "241  200:1001:1cbb30f8-8ddf-421a-8b3c-018631045ebc  200  1001  2017-02-02   \n",
      "242  200:1001:85d95010-63ac-4b4e-a984-0c3bdfaeac92  200  1001  2017-02-02   \n",
      "243  200:1001:657bf7cd-625a-47db-af23-9c70aea73b55  200  1001  2017-02-01   \n",
      "244  200:1001:5301e804-8282-47a1-97da-7922c24e0d00  200  1001  2017-02-03   \n",
      "245  200:1001:4ddd4eb0-9f14-4c3e-96ff-605703c46242  200  1001  2017-02-02   \n",
      "246  200:1001:f2ef8ad9-cce1-41d4-a8c2-fcebac0cfcf2  200  1001  2017-02-02   \n",
      "247  200:1001:697b6f94-2f7a-4e22-b039-3aa16dd1ea74  200  1001  2017-02-01   \n",
      "248  200:1001:cfe251ad-1280-4ab4-8c40-a13e12ee5132  200  1001  2017-02-01   \n",
      "249  200:1001:f4f8f99f-0555-4e8e-9ca2-6218d69ffffe  200  1001  2017-02-01   \n",
      "250  200:1001:471ca065-e854-4d5f-a6bb-31df5cc1d287  200  1001  2017-02-02   \n",
      "251  200:1001:8b4bf6b5-4de4-4203-ad7d-80eb103b1976  200  1001  2017-02-01   \n",
      "252  200:1001:13d3cfcc-fa15-478f-a8f5-85ed8d0e8ee4  200  1001  2017-02-01   \n",
      "253  200:1001:1a49885d-6ea7-461d-af62-2fd6ddd826ca  200  1001  2017-02-01   \n",
      "254  200:1001:755b3dfa-ebda-4ee6-aa88-2347cd58ebdd  200  1001  2017-02-02   \n",
      "255  200:1001:a3475e17-6f92-4cf3-be8c-ec46ca154028  200  1001  2017-02-02   \n",
      "256  200:1001:ea042aca-362b-4b8d-9d80-394514ca66e5  200  1001  2017-02-01   \n",
      "257  200:1001:6fd46fe3-60d7-4b0e-bc12-9f15588aa265  200  1001  2017-02-01   \n",
      "258  200:1001:a66dfb5c-859e-472b-b947-bdc060fdd0ea  200  1001  2017-02-02   \n",
      "259  200:1001:e4186ecd-afa7-49a1-8ac5-8d1e30dbe79c  200  1001  2017-02-02   \n",
      "260  200:1001:82245a08-afd4-4061-87a1-90796be56ba6  200  1001  2017-02-01   \n",
      "261  200:1001:b84d2f8d-65e4-4274-b43a-e2a5f844502c  200  1001  2017-02-01   \n",
      "\n",
      "             lt  \n",
      "0    1485980309  \n",
      "1    1485955778  \n",
      "2    1486028027  \n",
      "3    1486026868  \n",
      "4    1486004891  \n",
      "5    1486031370  \n",
      "6    1486006889  \n",
      "7    1485974263  \n",
      "8    1485939929  \n",
      "9    1486005121  \n",
      "10   1486031765  \n",
      "11   1485921495  \n",
      "12   1486046911  \n",
      "13   1485943713  \n",
      "14   1485954746  \n",
      "15   1485931535  \n",
      "16   1486035411  \n",
      "17   1485962345  \n",
      "18   1486049559  \n",
      "19   1486047651  \n",
      "20   1485967515  \n",
      "21   1485945044  \n",
      "22   1486029953  \n",
      "23   1486038993  \n",
      "24   1486005599  \n",
      "25   1486001168  \n",
      "26   1486047550  \n",
      "27   1486020328  \n",
      "28   1486021301  \n",
      "29   1486032860  \n",
      "..          ...  \n",
      "232  1486029942  \n",
      "233  1485920043  \n",
      "234  1486043808  \n",
      "235  1485932883  \n",
      "236  1485960386  \n",
      "237  1486020253  \n",
      "238  1485993262  \n",
      "239  1485910708  \n",
      "240  1485937774  \n",
      "241  1486034017  \n",
      "242  1486004525  \n",
      "243  1485947110  \n",
      "244  1486076970  \n",
      "245  1486024809  \n",
      "246  1485995523  \n",
      "247  1485946542  \n",
      "248  1485919515  \n",
      "249  1485923075  \n",
      "250  1486000886  \n",
      "251  1485930996  \n",
      "252  1485963685  \n",
      "253  1485920188  \n",
      "254  1486044790  \n",
      "255  1486026466  \n",
      "256  1485916265  \n",
      "257  1485922678  \n",
      "258  1486024655  \n",
      "259  1486025274  \n",
      "260  1485930480  \n",
      "261  1485934870  \n",
      "\n",
      "[262 rows x 5 columns]\n",
      "2017-06-28 18:39:23 union today acid_creatTime \n",
      "[]\n",
      "2017-06-28 18:39:23 union today uid_ByChannelcreatTime \n",
      "['/Users/tq/bigdatatest/pay/20170201_payLog.json']\n",
      "2017-06-28 18:39:24 start ltv\n",
      "2017-06-28 18:39:24 finish today data\n",
      "2017-06-28 18:39:24 start: 20170203\n",
      "2017-06-28 18:39:24 finish read original data\n",
      "2017-06-28 18:39:24 finish creat today jsonTable\n",
      "2017-06-28 18:39:24 finish IAP\n",
      "2017-06-28 18:39:24 finish uid Login\n",
      "2017-06-28 18:39:24 finish acid Login\n",
      "2017-06-28 18:39:24 finish all replace\n",
      "uid Creat Time:\n",
      "+--------------------+----------+----------+----+---+------------+--------------------+\n",
      "|                 uid|        lt|      date| sid|gid|          ch|             machine|\n",
      "+--------------------+----------+----------+----+---+------------+--------------------+\n",
      "|24e1a9d6-9929-45d...|1486081076|2017-02-03|1001|200|130134001300|        360 1505-A01|\n",
      "|504717e5-384b-466...|1486086575|2017-02-03|1001|200|130134001500|         Meizu PRO 5|\n",
      "|b4c12f87-3751-4c3...|1486089680|2017-02-03|1001|200|130134004600|YuLong Coolpad 86...|\n",
      "+--------------------+----------+----------+----+---+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "acid Creat Time:\n",
      "+--------------------+----------+----------+----+---+\n",
      "|                acid|        lt|      date| sid|gid|\n",
      "+--------------------+----------+----------+----+---+\n",
      "|200:1001:fd4b0463...|1486108150|2017-02-03|1001|200|\n",
      "|200:1001:bdfe4975...|1486129321|2017-02-03|1001|200|\n",
      "|200:1001:aa9e5a1d...|1486129339|2017-02-03|1001|200|\n",
      "+--------------------+----------+----------+----+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+----------+----------+----+---+\n",
      "|                 uid|        lt|      date| sid|gid|\n",
      "+--------------------+----------+----------+----+---+\n",
      "|e1ff3ae0-b644-4d2...|1486105783|2017-02-03|1001|200|\n",
      "|881c7b0f-c5e5-4a1...|1486143212|2017-02-04|1001|200|\n",
      "|dedae4ee-6774-400...|1486101206|2017-02-03|1001|200|\n",
      "+--------------------+----------+----------+----+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "payLog:\n",
      "+--------------------+--------------------+---+----+------------+-------+------+------+--------------+----------+----------+\n",
      "|                 uid|                acid|gid| sid|          ch|corpLvl|vipLvl|payNum|       payTime|   LogTime|      date|\n",
      "+--------------------+--------------------+---+----+------------+-------+------+------+--------------+----------+----------+\n",
      "|046e9dba-79c4-4d3...|200:1001:046e9dba...|200|1001|130134001500|     79|    14|  3000|20170203091047|1486084263|2017-02-03|\n",
      "|d4c87387-fb97-49f...|200:1001:d4c87387...|200|1001|130134005400|     25|     2|  3000|20170203092258|1486085100|2017-02-03|\n",
      "|e9fd2261-30d4-470...|200:1001:e9fd2261...|200|1001|130134001300|     25|     3|  3000|20170203104537|1486089986|2017-02-03|\n",
      "+--------------------+--------------------+---+----+------------+-------+------+------+--------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2017-06-28 18:39:25 finish today data\n",
      "2017-06-28 18:39:25 union today uid_creatTime\n",
      "['/Users/tq/bigdatatest/uid_creatTime.json']\n",
      "2017-06-28 18:39:27 union today uid_creatTime\n",
      "['/Users/tq/bigdatatest/acid_creatTime.json']\n",
      "acidListMerge                                               acid  gid   sid        date  \\\n",
      "0    200:1001:9ad0faf7-feac-4d22-b750-aab9fdd9d6d1  200  1001  2017-02-02   \n",
      "1    200:1001:cdf8010a-e27e-4407-94d1-c050067fa578  200  1001  2017-02-01   \n",
      "2    200:1001:dc8f1d50-a421-4adb-a5f4-2c606098ef4f  200  1001  2017-02-03   \n",
      "3    200:1001:5c4d7820-e97d-4ab6-b194-2f2fcd6c425d  200  1001  2017-02-02   \n",
      "4    200:1001:d9fde481-e002-43a0-8f04-f25276832171  200  1001  2017-02-02   \n",
      "5    200:1001:ac0c133b-6749-4add-b381-fb19aabd64da  200  1001  2017-02-02   \n",
      "6    200:1001:3f1dc78e-b5c2-48fd-9b57-5ec99e973148  200  1001  2017-02-02   \n",
      "7    200:1001:20db349a-3e2f-45c9-89ae-bfd1f93ea5d6  200  1001  2017-02-02   \n",
      "8    200:1001:3f20c3a9-fa40-4cf7-b152-8c0224081e49  200  1001  2017-02-02   \n",
      "9    200:1001:49b73e60-5407-48dd-9d37-021885f2d357  200  1001  2017-02-03   \n",
      "10   200:1001:beb74c9d-808b-40a9-a0f9-c9bcf1c88caf  200  1001  2017-02-01   \n",
      "11   200:1001:85e251b6-df43-4c52-a01a-a3929cea74b1  200  1001  2017-02-03   \n",
      "12   200:1001:b937422c-217f-4a19-aaf5-06abdf860374  200  1001  2017-02-02   \n",
      "13   200:1001:f57301fc-07da-47fb-a983-54246ad0fea6  200  1001  2017-02-02   \n",
      "14   200:1001:0affc2c9-6eaa-46aa-9faa-0fc01c132970  200  1001  2017-02-01   \n",
      "15   200:1001:8056ce7d-86dd-4e7c-847c-370f2d742232  200  1001  2017-02-02   \n",
      "16   200:1001:ff0da345-5619-4d22-92f4-ec9111a6e394  200  1001  2017-02-03   \n",
      "17   200:1001:5e3f1cab-0f7a-44cc-8fb0-094b720b6339  200  1001  2017-02-03   \n",
      "18   200:1001:be4fd2b7-c8e8-4046-b9c0-02046a46b246  200  1001  2017-02-03   \n",
      "19   200:1001:73b6d305-baea-4aed-bd1a-036b37c63c0f  200  1001  2017-02-01   \n",
      "20   200:1001:fc647982-e056-4787-a1b5-b05c17d3add9  200  1001  2017-02-01   \n",
      "21   200:1001:ea7a0a7d-4756-4fe0-a1c3-2d9300004df2  200  1001  2017-02-01   \n",
      "22   200:1001:aa9e5a1d-d912-4ecb-b330-e132d57f1616  200  1001  2017-02-03   \n",
      "23   200:1001:9cbc9129-79dc-4ffc-8b44-96677db9dc8d  200  1001  2017-02-03   \n",
      "24   200:1001:9156b312-d191-46cf-bc4c-f8177307f2c3  200  1001  2017-02-02   \n",
      "25   200:1001:0f6945ae-c903-43cd-bcaa-6884487efc60  200  1001  2017-02-01   \n",
      "26   200:1001:b4aace06-4e67-4666-b1c5-f3695359567b  200  1001  2017-02-02   \n",
      "27   200:1001:326730d8-7eee-4244-85da-5c1b2404b215  200  1001  2017-02-02   \n",
      "28   200:1001:a7e7412d-864c-43ae-8183-8d4beba7ad3c  200  1001  2017-02-02   \n",
      "29   200:1001:3b18975a-6e7c-4d7b-9cd5-f726df8cac65  200  1001  2017-02-01   \n",
      "..                                             ...  ...   ...         ...   \n",
      "338  200:1001:ffe2e052-8260-45e6-bff7-96913bfbb470  200  1001  2017-02-03   \n",
      "339  200:1001:9d8e1751-26e5-4793-99cd-cf49b0a88b7e  200  1001  2017-02-01   \n",
      "340  200:1001:763f72a2-1bff-4990-9706-34d447073e45  200  1001  2017-02-03   \n",
      "341  200:1001:da7ba4f4-3458-4610-b9b7-fc90c9504148  200  1001  2017-02-03   \n",
      "342  200:1001:1cbb30f8-8ddf-421a-8b3c-018631045ebc  200  1001  2017-02-02   \n",
      "343  200:1001:85d95010-63ac-4b4e-a984-0c3bdfaeac92  200  1001  2017-02-02   \n",
      "344  200:1001:657bf7cd-625a-47db-af23-9c70aea73b55  200  1001  2017-02-01   \n",
      "345  200:1001:5301e804-8282-47a1-97da-7922c24e0d00  200  1001  2017-02-03   \n",
      "346  200:1001:4ddd4eb0-9f14-4c3e-96ff-605703c46242  200  1001  2017-02-02   \n",
      "347  200:1001:f2ef8ad9-cce1-41d4-a8c2-fcebac0cfcf2  200  1001  2017-02-02   \n",
      "348  200:1001:697b6f94-2f7a-4e22-b039-3aa16dd1ea74  200  1001  2017-02-01   \n",
      "349  200:1001:cfe251ad-1280-4ab4-8c40-a13e12ee5132  200  1001  2017-02-01   \n",
      "350  200:1001:f4f8f99f-0555-4e8e-9ca2-6218d69ffffe  200  1001  2017-02-01   \n",
      "351  200:1001:674a4083-b3aa-42a4-b323-5f5c6ce3a795  200  1001  2017-02-03   \n",
      "352  200:1001:1c957598-7c4d-40ff-b388-0007bb8f0b48  200  1001  2017-02-04   \n",
      "353  200:1001:471ca065-e854-4d5f-a6bb-31df5cc1d287  200  1001  2017-02-02   \n",
      "354  200:1001:e2c75ada-6baf-44dd-a992-63e4849b3e5a  200  1001  2017-02-03   \n",
      "355  200:1001:8b4bf6b5-4de4-4203-ad7d-80eb103b1976  200  1001  2017-02-01   \n",
      "356  200:1001:13d3cfcc-fa15-478f-a8f5-85ed8d0e8ee4  200  1001  2017-02-01   \n",
      "357  200:1001:1a49885d-6ea7-461d-af62-2fd6ddd826ca  200  1001  2017-02-01   \n",
      "358  200:1001:65bee98e-e680-4fec-a377-41f0bdda79c1  200  1001  2017-02-03   \n",
      "359  200:1001:755b3dfa-ebda-4ee6-aa88-2347cd58ebdd  200  1001  2017-02-02   \n",
      "360  200:1001:a3475e17-6f92-4cf3-be8c-ec46ca154028  200  1001  2017-02-02   \n",
      "361  200:1001:ea042aca-362b-4b8d-9d80-394514ca66e5  200  1001  2017-02-01   \n",
      "362  200:1001:6fd46fe3-60d7-4b0e-bc12-9f15588aa265  200  1001  2017-02-01   \n",
      "363  200:1001:a66dfb5c-859e-472b-b947-bdc060fdd0ea  200  1001  2017-02-02   \n",
      "364  200:1001:e4186ecd-afa7-49a1-8ac5-8d1e30dbe79c  200  1001  2017-02-02   \n",
      "365  200:1001:fd4b0463-63f1-4690-becb-46ad1767a9f4  200  1001  2017-02-03   \n",
      "366  200:1001:82245a08-afd4-4061-87a1-90796be56ba6  200  1001  2017-02-01   \n",
      "367  200:1001:b84d2f8d-65e4-4274-b43a-e2a5f844502c  200  1001  2017-02-01   \n",
      "\n",
      "             lt  \n",
      "0    1485980309  \n",
      "1    1485955778  \n",
      "2    1486105023  \n",
      "3    1486028027  \n",
      "4    1486026868  \n",
      "5    1486004891  \n",
      "6    1486031370  \n",
      "7    1486006889  \n",
      "8    1485974263  \n",
      "9    1486099258  \n",
      "10   1485939929  \n",
      "11   1486117709  \n",
      "12   1486005121  \n",
      "13   1486031765  \n",
      "14   1485921495  \n",
      "15   1486046911  \n",
      "16   1486128989  \n",
      "17   1486121596  \n",
      "18   1486129930  \n",
      "19   1485943713  \n",
      "20   1485954746  \n",
      "21   1485931535  \n",
      "22   1486129339  \n",
      "23   1486102431  \n",
      "24   1486035411  \n",
      "25   1485962345  \n",
      "26   1486049559  \n",
      "27   1486047651  \n",
      "28   1485967515  \n",
      "29   1485945044  \n",
      "..          ...  \n",
      "338  1486101178  \n",
      "339  1485937774  \n",
      "340  1486116022  \n",
      "341  1486107184  \n",
      "342  1486034017  \n",
      "343  1486004525  \n",
      "344  1485947110  \n",
      "345  1486076970  \n",
      "346  1486024809  \n",
      "347  1485995523  \n",
      "348  1485946542  \n",
      "349  1485919515  \n",
      "350  1485923075  \n",
      "351  1486124008  \n",
      "352  1486142336  \n",
      "353  1486000886  \n",
      "354  1486105284  \n",
      "355  1485930996  \n",
      "356  1485963685  \n",
      "357  1485920188  \n",
      "358  1486099656  \n",
      "359  1486044790  \n",
      "360  1486026466  \n",
      "361  1485916265  \n",
      "362  1485922678  \n",
      "363  1486024655  \n",
      "364  1486025274  \n",
      "365  1486108150  \n",
      "366  1485930480  \n",
      "367  1485934870  \n",
      "\n",
      "[368 rows x 5 columns]\n",
      "2017-06-28 18:39:30 union today acid_creatTime \n",
      "[]\n",
      "2017-06-28 18:39:31 union today uid_ByChannelcreatTime \n",
      "['/Users/tq/bigdatatest/pay/20170201_payLog.json', '/Users/tq/bigdatatest/pay/20170202_payLog.json']\n",
      "2017-06-28 18:39:31 start ltv\n",
      "2017-06-28 18:39:31 finish today data\n",
      "2017-06-28 18:39:31 start: 20170204\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u'Path does not exist: file:/Users/tq/bigdatatest/20170204.log;'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-2db2deec8c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"start:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_file_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"logics3/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"finish read original data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RawTable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tq/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tq/anaconda/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tq/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Path does not exist: file:/Users/tq/bigdatatest/20170204.log;'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import boto\n",
    "from boto import s3\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import StringIO\n",
    "import pytz\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, Integer, String, create_engine,Float\n",
    "from sqlalchemy.orm import scoped_session, sessionmaker\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "now_utc = datetime.datetime.utcnow()\n",
    "local_tz = pytz.timezone('Asia/Shanghai')\n",
    "now_utc = pytz.utc.localize(now_utc)\n",
    "local_time = now_utc.astimezone(local_tz)\n",
    "current_date = local_time.strftime('%Y%m%d')\n",
    "\n",
    "islocal = True\n",
    "localPath = \"/Users/tq/bigdatatest/\"\n",
    "# targetpay_path = 'uidcreattime-csv/pay/'\n",
    "# target_path = 'uidcreattime-csv'\n",
    "# firstDay = datetime.date(2016, 11, 10)\n",
    "\n",
    "firstDay = datetime.date(2017, 3, 7)\n",
    "target_path = '/Users/tq/bigdatatest/'\n",
    "targetpay_path = 'Users/tq/bigdatatest/pay/' #本地\n",
    "def getFalesChannel():\n",
    "    false_channel = [\n",
    "    \"130134201\",\n",
    "    \"110134101106.0\",\n",
    "    \"13013412700\",\n",
    "    \"13013419400\",\n",
    "    \"130134164\",\n",
    "    \"47\",\n",
    "    \"13013415800\",\n",
    "    \"1301343257\",\n",
    "    \"13013413400\",\n",
    "    \"131\",\n",
    "    \"142\",\n",
    "    \"145\",\n",
    "    \"180\",\n",
    "    \"188\",\n",
    "    \"196\",\n",
    "    \"204\",\n",
    "    \"210\",\n",
    "    \"213\",\n",
    "    \"215\",\n",
    "    \"216\",\n",
    "    \"223\",\n",
    "    \"234\",\n",
    "    \"250\",\n",
    "    \"259\",\n",
    "    \"262\",\n",
    "    \"263\",\n",
    "    \"265\",\n",
    "    \"292\",\n",
    "    \"296\",\n",
    "    \"298\",\n",
    "    \"306\",\n",
    "    \"309\",\n",
    "    \"313\",\n",
    "    \"1301341500\",\n",
    "    \"1301341232\",\n",
    "    \"13013416000\",\n",
    "    \"13013411000\",\n",
    "    \"1301344600\",\n",
    "    \"1301345600\",\n",
    "    \"13013412000\",\n",
    "    \"1301341272\",\n",
    "    \"13013415900\",\n",
    "    \"130134137\",\n",
    "    \"1301371335\",\n",
    "    \"130134126\",\n",
    "    \"1301343900\",\n",
    "    \"13013415200\",\n",
    "    \"13013423200\",\n",
    "    \"1301343234\",\n",
    "    \"130134312108\",\n",
    "    \"1301341300\",\n",
    "    \"130134312173\",\n",
    "    \"5000\",\n",
    "    \"130134159\",\n",
    "    \"1301343800\",\n",
    "    \"1301345400\",\n",
    "    \"1301341\",\n",
    "    \"130134144\",\n",
    "    \"13013443300\",\n",
    "    \"268\",\n",
    "    \"350\",\n",
    "    \"389\"\n",
    "]\n",
    "    return false_channel\n",
    "\n",
    "def getTrueChannel():\n",
    "    true_channel = [\n",
    "        \"130134000201\",\n",
    "        \"110134101106\",\n",
    "        \"130134012700\",\n",
    "        \"130134019400\",\n",
    "        \"130134000164\",\n",
    "        \"47\",\n",
    "        \"130134015800\",\n",
    "        \"130134003257\",\n",
    "        \"130134013400\",\n",
    "        \"131\",\n",
    "        \"142\",\n",
    "        \"145\",\n",
    "        \"180\",\n",
    "        \"188\",\n",
    "        \"196\",\n",
    "        \"204\",\n",
    "        \"210\",\n",
    "        \"213\",\n",
    "        \"215\",\n",
    "        \"216\",\n",
    "        \"223\",\n",
    "        \"234\",\n",
    "        \"250\",\n",
    "        \"259\",\n",
    "        \"262\",\n",
    "        \"263\",\n",
    "        \"265\",\n",
    "        \"292\",\n",
    "        \"296\",\n",
    "        \"298\",\n",
    "        \"306\",\n",
    "        \"309\",\n",
    "        \"313\",\n",
    "        \"130134001500\",\n",
    "        \"130134001232\",\n",
    "        \"130134016000\",\n",
    "        \"130134011000\",\n",
    "        \"130134004600\",\n",
    "        \"130134005600\",\n",
    "        \"130134012000\",\n",
    "        \"130134001272\",\n",
    "        \"130134015900\",\n",
    "        \"130134000137\",\n",
    "        \"130134001335\",\n",
    "        \"130134000126\",\n",
    "        \"130134003900\",\n",
    "        \"130134015200\",\n",
    "        \"130134023200\",\n",
    "        \"130134003234\",\n",
    "        \"130134312108\",\n",
    "        \"130134001300\",\n",
    "        \"130134312173\",\n",
    "        \"5000\",\n",
    "        \"130134000159\",\n",
    "        \"130134003800\",\n",
    "        \"130134005400\",\n",
    "        \"130134000001\",\n",
    "        \"130134000144\",\n",
    "        \"130134043300\",\n",
    "        \"268\",\n",
    "        \"350\",\n",
    "        \"389\"\n",
    "    ]\n",
    "    return true_channel\n",
    "\n",
    "def getDBDriver(tableName):\n",
    "    return spark.read.format(\"jdbc\").option(\"url\", url).option(\"driver\", \"com.mysql.jdbc.Driver\").option(\n",
    "            \"dbtable\", tableName).option(\"user\", properties['user']).option(\"password\", properties['password']).load()\n",
    "\n",
    "\n",
    "def get_s3_list(daterange,prefixpath):\n",
    "    prefix = prefixpath\n",
    "    total_size = 0\n",
    "    REGION = \"cn-north-1\"\n",
    "    conn = s3.connect_to_region(REGION)\n",
    "    bucket = conn.lookup('prodlog')\n",
    "    ret = []\n",
    "    if bucket:\n",
    "        for k in bucket.list(prefix=prefix):\n",
    "            if k.size <= 0:\n",
    "                continue\n",
    "            logsp = k.name.split('.')\n",
    "            a = logsp[-4:-1]\n",
    "            a.reverse()\n",
    "            dt = ''.join(a)\n",
    "            if dt == daterange:\n",
    "                total_size += k.size\n",
    "                ret.append('s3://prodlog/' + k.name)\n",
    "                print('s3://prodlog/' + k.name, ''.join(a))\n",
    "    print('total:%d' % (total_size / 1024.0 / 1024.0 / 1024.0))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_local_list(daterange):\n",
    "        return \"/Users/tq/bigdatatest/%s.log\"%daterange\n",
    "\n",
    "def get_file_list(daterange,prefixpath):\n",
    "    if islocal:\n",
    "\n",
    "        return get_local_list(daterange)\n",
    "    else:\n",
    "        return get_s3_list(daterange,prefixpath)\n",
    "\n",
    "\n",
    "\n",
    "def get_s3_tempdata(dataName):\n",
    "    prefix = \"uidcreattime-csv/\"\n",
    "    total_size = 0\n",
    "    REGION = \"cn-north-1\"\n",
    "    conn = s3.connect_to_region(REGION)\n",
    "    bucket = conn.lookup('prodlog')\n",
    "    ret = []\n",
    "    if bucket:\n",
    "        for k in bucket.list(prefix=prefix):\n",
    "            if k.size <= 0:\n",
    "                continue\n",
    "            if dataName == \"payLog\":\n",
    "                logsp = k.name.split('/')\n",
    "                a = logsp[len(logsp) - 1]\n",
    "                if dataName in a:\n",
    "                    total_size += k.size\n",
    "                    ret.append('s3://prodlog/' + k.name)\n",
    "                    print('test :s3://prodlog/' + k.name, ''.join(a))\n",
    "            else:\n",
    "                logsp = k.name.split('/')\n",
    "                a = logsp[len(logsp)-1]\n",
    "                csvName = a.split('.')[0]\n",
    "                if csvName == dataName:\n",
    "                    total_size += k.size\n",
    "                    ret.append('s3://prodlog/' + k.name)\n",
    "                    print('test :s3://prodlog/' + k.name, ''.join(a))\n",
    "    print('total:%d' % (total_size / 1024.0 / 1024.0 / 1024.0))\n",
    "    return ret\n",
    "\n",
    "def getTempDataList(dataName):\n",
    "    if islocal:\n",
    "        return search(localPath,dataName)\n",
    "    else:\n",
    "        return get_s3_tempdata(dataName)\n",
    "\n",
    "def search(path, word):\n",
    "    name = []\n",
    "    if word == \"payLog\":\n",
    "        for filename in os.listdir(\"/Users/tq/bigdatatest/pay\"):\n",
    "            fp = os.path.join(\"/Users/tq/bigdatatest/pay\", filename)\n",
    "            if word in filename:\n",
    "                name.append(fp)\n",
    "    else:\n",
    "        for filename in os.listdir(path):\n",
    "            fp = os.path.join(path, filename)\n",
    "            if os.path.isfile(fp) and word in filename:\n",
    "                name.append(fp)\n",
    "    print name\n",
    "    return name\n",
    "\n",
    "def min_time(x, y):\n",
    "    if int(x.lt) > int(y.lt):\n",
    "        return y\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def upload_to_s3(pandasdf, region, bucket_name, s3_filepath):\n",
    "    for column in pandasdf.columns:\n",
    "        for idx in pandasdf[column].index:\n",
    "            x = pandasdf.get_value(idx, column)\n",
    "            try:\n",
    "                x = unicode(x.encode('utf-8', 'ignore'), errors='ignore') if type(x) == unicode else unicode(str(x),errors='ignore')\n",
    "                pandasdf.set_value(idx, column, x)\n",
    "            except Exception:\n",
    "                print\n",
    "                'encoding error: {0} {1}'.format(idx, column)\n",
    "                pandasdf.set_value(idx, column, '')\n",
    "                continue\n",
    "    json_buffer = StringIO.StringIO()\n",
    "    pandasdf.to_json(json_buffer,orient='records')\n",
    "    conn = boto.s3.connect_to_region(region)\n",
    "    bucket = conn.get_bucket(bucket_name)\n",
    "    full_key_name = s3_filepath\n",
    "    k = bucket.new_key(full_key_name)\n",
    "    k.set_contents_from_string(json_buffer.getvalue())\n",
    "    return None\n",
    "\n",
    "def upload_to_local(pandasdf, s3_filepath):\n",
    "#     for column in pandasdf.columns:\n",
    "#         for idx in pandasdf[column].index:\n",
    "#             x = pandasdf.get_value(idx, column)\n",
    "#             try:\n",
    "#                 x = unicode(x.encode('utf-8', 'ignore'), errors='ignore') if type(x) == unicode else unicode(str(x),errors='ignore')\n",
    "#                 pandasdf.set_value(idx, column, x)\n",
    "#             except Exception:\n",
    "#                 print\n",
    "#                 'encoding error: {0} {1}'.format(idx, column)\n",
    "#                 pandasdf.set_value(idx, column, '')\n",
    "#                 continue\n",
    "    pandasdf.to_json(s3_filepath,orient='records')\n",
    "    return None\n",
    "\n",
    "\n",
    "def upload(pandasdf, region, bucket_name, s3_filepath):\n",
    "    if islocal:\n",
    "        upload_to_local(pandasdf,s3_filepath)\n",
    "    else:\n",
    "        upload_to_s3(pandasdf, region, bucket_name, s3_filepath)\n",
    "\n",
    "properties = {\n",
    "    \"user\": \"test1\",\n",
    "    \"password\": \"QmPhaQ8hYsxx\"\n",
    "}\n",
    "uidTableName = \"uid_creatTime\"\n",
    "acidTableName = \"account_creatTime\"\n",
    "payTableName = \"account_payTime\"\n",
    "url = \"jdbc:mysql://54.223.192.252:3306/test\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "dbname='mysql+mysqlconnector://test1:QmPhaQ8hYsxx@54.223.192.252:3306/test'\n",
    "engine = create_engine(dbname, echo=False)\n",
    "DBSession = scoped_session(sessionmaker(bind=engine))\n",
    "\n",
    "\n",
    "class LTVByGid(Base):\n",
    "    __tablename__ = \"ltv_byGid\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    gid = Column(String(255))\n",
    "    creat_time = Column(String(255))\n",
    "    people_count = Column(String(255))\n",
    "    days = Column(String(255))\n",
    "    consume = Column(Float, nullable=True)\n",
    "\n",
    "class LTVBySid(Base):\n",
    "    __tablename__ = \"ltv_bySid\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    sid = Column(String(255))\n",
    "    creat_time = Column(String(255))\n",
    "    people_count = Column(String(255))\n",
    "    days = Column(String(255))\n",
    "    consume = Column(Float, nullable=True)\n",
    "class LTVByCh(Base):\n",
    "    __tablename__ = \"ltv_byCh\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    ch = Column(String(255))\n",
    "    creat_time = Column(String(255))\n",
    "    people_count = Column(String(255))\n",
    "    days = Column(String(255))\n",
    "    consume = Column(Float, nullable=True)\n",
    "\n",
    "def ltv_byGid_sqlalchemy_orm(df):\n",
    "    # init_sqlalchemy()\n",
    "    session = DBSession()\n",
    "    x = []\n",
    "    for dfdate in df.values:\n",
    "        customer = LTVByGid()\n",
    "        customer.gid = str(dfdate[0])\n",
    "        customer.creat_time = str(dfdate[3])\n",
    "        customer.people_count = str(dfdate[2])\n",
    "        customer.days = str(dfdate[4])\n",
    "        if dfdate[1] is None:\n",
    "            customer.consume = float(0)\n",
    "        else:\n",
    "            customer.consume = float(dfdate[1]) / float(dfdate[2]) / 100\n",
    "        x.append(customer)\n",
    "    session.add_all(x)\n",
    "    session.commit()\n",
    "\n",
    "def ltv_bySid_sqlalchemy_orm(df):\n",
    "    # init_sqlalchemy()\n",
    "    session = DBSession()\n",
    "    x = []\n",
    "    for dfdate in df.values:\n",
    "        customer = LTVBySid()\n",
    "        customer.sid = str(dfdate[0])\n",
    "        customer.creat_time = str(dfdate[3])\n",
    "        customer.people_count = str(dfdate[2])\n",
    "        customer.days = str(dfdate[4])\n",
    "        if dfdate[1] is None:\n",
    "            customer.consume = float(0)\n",
    "        else:\n",
    "            customer.consume = float(dfdate[1]) / float(dfdate[2]) / 100\n",
    "        x.append(customer)\n",
    "    session.add_all(x)\n",
    "    session.commit()\n",
    "\n",
    "def ltv_byCh_sqlalchemy_orm(df):\n",
    "    # init_sqlalchemy()\n",
    "    session = DBSession()\n",
    "    x = []\n",
    "    for dfdate in df.values:\n",
    "        customer = LTVByCh()\n",
    "        customer.ch = str(dfdate[0])\n",
    "        customer.creat_time = str(dfdate[3])\n",
    "        customer.people_count = str(dfdate[2])\n",
    "        customer.days = str(dfdate[4])\n",
    "        if dfdate[1] is None:\n",
    "            customer.consume = float(0)\n",
    "        else:\n",
    "            customer.consume = float(dfdate[1]) / float(dfdate[2]) / 100\n",
    "        x.append(customer)\n",
    "    session.add_all(x)\n",
    "    session.commit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    begin = datetime.date(2017, 2, 1)\n",
    "    end = datetime.date(2017, 6, 21)\n",
    "\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"creat spark session\"\n",
    "    spark = SparkSession \\\n",
    "        .builder.enableHiveSupport() \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    for i in range((end - begin).days + 1):\n",
    "        xx = str(begin + datetime.timedelta(days=i))\n",
    "        pp = xx.split('-')\n",
    "        day = \"\".join(pp)\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))),\"start:\",day\n",
    "\n",
    "        df = spark.read.json(get_file_list(day,\"logics3/\"))\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"finish read original data\"\n",
    "        df.createOrReplaceTempView('RawTable')\n",
    "        preparetable = 'RawTable'\n",
    "\n",
    "        spark.sql(u'''\n",
    "        SELECT\n",
    "            *\n",
    "        FROM %s\n",
    "        WHERE accountid is not null\n",
    "        AND type_name!=\"CorpLevelChg\"\n",
    "        ''' % preparetable).createOrReplaceTempView('jsonTable')\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"finish creat today jsonTable\"\n",
    "        # 获取当日log里的支付记录\n",
    "        paylog_df = spark.sql(u\"\"\"\n",
    "        SELECT\n",
    "            jsonTable.userid as uid,\n",
    "            jsonTable.accountid AS acid,\n",
    "            jsonTable.gid AS gid,\n",
    "            jsonTable.sid AS sid,\n",
    "            jsonTable.channel AS ch,\n",
    "            jsonTable.corplvl AS corpLvl,\n",
    "            jsonTable.info.VipLvl AS vipLvl,\n",
    "            jsonTable.info.Money AS payNum,\n",
    "            jsonTable.info.PayTime AS payTime,\n",
    "            SUBSTRING(jsonTable.logtime,0,10) AS LogTime,\n",
    "            CAST(CAST(jsonTable.utc8 as DATE) as varchar(10)) AS date\n",
    "        FROM jsonTable\n",
    "        WHERE type_name = \"IAP\"\n",
    "        \"\"\")\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"finish IAP\"\n",
    "        # 获取当日log里的最早的一次登录\n",
    "        uidlogByChannel_df = spark.sql(u'''\n",
    "                SELECT\n",
    "                    t.uid AS uid,\n",
    "                    SUBSTRING(t.lt,0,10) AS lt,\n",
    "                    CAST(CAST(jsonTable.utc8 as DATE) as varchar(10)) AS date,\n",
    "                    jsonTable.sid AS sid,\n",
    "                    jsonTable.gid AS gid,\n",
    "                    jsonTable.channel AS ch,\n",
    "                    jsonTable.info.MachineType AS machine\n",
    "                FROM (\n",
    "                    SELECT\n",
    "                        userid AS uid,\n",
    "                        min(logtime) AS lt\n",
    "                    FROM jsonTable\n",
    "                    WHERE type_name=\"Login\"\n",
    "                    GROUP BY userid\n",
    "                ) t\n",
    "                LEFT JOIN jsonTable ON jsonTable.userid = t.uid AND jsonTable.logtime = t.lt\n",
    "            ''')\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"finish uid Login\"\n",
    "\n",
    "        #读取createProfile\n",
    "        uidlogNoChannel_df = spark.sql(u'''\n",
    "                SELECT\n",
    "                    t.uid AS uid,\n",
    "                    SUBSTRING(t.lt,0,10) AS lt,\n",
    "                    CAST(CAST(jsonTable.utc8 as DATE) as varchar(10)) AS date,\n",
    "                    jsonTable.sid AS sid,\n",
    "                    jsonTable.gid AS gid\n",
    "                FROM (\n",
    "                    SELECT\n",
    "                        userid AS uid,\n",
    "                        min(logtime) AS lt\n",
    "                    FROM jsonTable\n",
    "                    WHERE type_name=\"CreateProfile\"\n",
    "                    GROUP BY userid\n",
    "                ) t\n",
    "                LEFT JOIN jsonTable ON jsonTable.userid = t.uid AND jsonTable.logtime = t.lt\n",
    "            ''')\n",
    "\n",
    "        acidlogNoChannel_df = spark.sql(u'''\n",
    "                SELECT\n",
    "                    t.acid AS acid,\n",
    "                    SUBSTRING(t.lt,0,10) AS lt,\n",
    "                    CAST(CAST(jsonTable.utc8 as DATE) as varchar(10)) AS date,\n",
    "                    jsonTable.sid AS sid,\n",
    "                    jsonTable.gid AS gid\n",
    "                FROM (\n",
    "                    SELECT\n",
    "                        accountid AS acid,\n",
    "                        min(logtime) AS lt\n",
    "                    FROM jsonTable\n",
    "                    WHERE type_name=\"CreateProfile\"\n",
    "                    GROUP BY accountid\n",
    "                ) t\n",
    "                LEFT JOIN jsonTable ON jsonTable.accountid = t.acid AND jsonTable.logtime = t.lt\n",
    "                ''')\n",
    "\n",
    "        # 获取当日log里分服务器玩家角色的第一次登录\n",
    "        acidlog_df = spark.sql(u'''\n",
    "                SELECT\n",
    "                    t.acid AS acid,\n",
    "                    SUBSTRING(t.lt,0,10) AS lt,\n",
    "                    CAST(CAST(jsonTable.utc8 as DATE) as varchar(10)) AS date,\n",
    "                    jsonTable.sid AS sid,\n",
    "                    jsonTable.gid AS gid,\n",
    "                    jsonTable.channel AS ch,\n",
    "                    jsonTable.info.MachineType AS machine\n",
    "                FROM (\n",
    "                    SELECT\n",
    "                        accountid AS acid,\n",
    "                        min(logtime) AS lt\n",
    "                    FROM jsonTable\n",
    "                    WHERE type_name=\"Login\"\n",
    "                    GROUP BY accountid\n",
    "                ) t\n",
    "                LEFT JOIN jsonTable ON jsonTable.accountid = t.acid AND jsonTable.logtime = t.lt\n",
    "                ''')\n",
    "\n",
    "\n",
    "\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"finish acid Login\"\n",
    "\n",
    "\n",
    "        false_channel = getFalesChannel()\n",
    "        true_channel = getTrueChannel()\n",
    "        # 替换channel\n",
    "        uid_creatTimeBychannelLog = uidlogByChannel_df.replace(false_channel, true_channel, \"ch\")\n",
    "\n",
    "        paylog_data = paylog_df.replace(false_channel, true_channel, \"ch\")\n",
    "\n",
    "\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"finish all replace\"\n",
    "        print \"uid Creat Time:\"\n",
    "        uid_creatTimeBychannelLog.show(3)\n",
    "        print \"acid Creat Time:\"\n",
    "        acidlogNoChannel_df.show(3)\n",
    "        uidlogNoChannel_df.show(3)\n",
    "        print \"payLog:\"\n",
    "        paylog_data.show(3)\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"finish today data\"\n",
    "        # S3上读取中间数据\n",
    "        #uid->creat time\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"union today uid_creatTime\"\n",
    "\n",
    "        #uid无Channel\n",
    "        uidList = getTempDataList(\"uid_creatTime\")\n",
    "        if len(uidList) > 0:\n",
    "            df_old_uid = spark.read.json(uidList)\n",
    "            df_old_uid.createOrReplaceTempView('uidolddf')\n",
    "            \n",
    "            uidlogNoChannel_df.createOrReplaceTempView('uidnochanneldf')\n",
    "            df_temp_uid_temp = spark.sql(\n",
    "                '''\n",
    "                    SELECT \n",
    "                        uid,\n",
    "                        gid,\n",
    "                        sid,\n",
    "                        date,\n",
    "                        lt\n",
    "                    FROM uidolddf\n",
    "                '''\n",
    "            )\n",
    "            \n",
    "            uidlogNoChannel_df_temp = spark.sql(\n",
    "                '''\n",
    "                    SELECT \n",
    "                        uid,\n",
    "                        gid,\n",
    "                        sid,\n",
    "                        date,\n",
    "                        lt\n",
    "                    FROM uidnochanneldf\n",
    "                '''\n",
    "            )\n",
    "            \n",
    "            union_df = df_temp_uid_temp.union(uidlogNoChannel_df_temp)\n",
    "            \n",
    "            final_uid_Result = union_df.rdd.map(lambda x: (x.uid, x)).reduceByKey(min_time).values()\n",
    "#             schema =  StructType([\n",
    "#                 StructField (\"uid\" , StringType(), True), \n",
    "#                 StructField(\"gid\" , IntegerType(), True),\n",
    "#                 StructField(\"sid\" , IntegerType(), True),\n",
    "#                 StructField(\"date\" , StringType(), True),\n",
    "#                 StructField(\"lt\" , StringType(), True)])\n",
    "            \n",
    "            final_uid_Result_fream = spark.createDataFrame(final_uid_Result)\n",
    "            \n",
    "            final_uid_Result_fream.createOrReplaceTempView('uidlog')\n",
    "\n",
    "            xx = final_uid_Result_fream.toPandas()\n",
    "            upload(xx, 'cn-north-1', \"prodlog\", '/%s/uid_creatTime.json' % target_path)\n",
    "        else:\n",
    "            uidlogNoChannel_df.createOrReplaceTempView('uidlog')\n",
    "            upload(uidlogNoChannel_df.toPandas(),'cn-north-1', \"prodlog\", '/%s/uid_creatTime.json' % target_path)\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"union today uid_creatTime\"\n",
    "\n",
    "\n",
    "        #acid无channel\n",
    "        acidList = getTempDataList(\"acid_creatTime\")\n",
    "        if len(acidList) > 0:\n",
    "            df_temp_acid = spark.read.json(acidList)\n",
    "            \n",
    "            df_temp_acid.createOrReplaceTempView('acidolddf')\n",
    "            \n",
    "            acidlogNoChannel_df.createOrReplaceTempView('acidnochanneldf')\n",
    "            df_temp_uid_temp = spark.sql(\n",
    "                '''\n",
    "                    SELECT \n",
    "                        acid,\n",
    "                        gid,\n",
    "                        sid,\n",
    "                        date,\n",
    "                        lt\n",
    "                    FROM acidolddf\n",
    "                '''\n",
    "            )\n",
    "            \n",
    "            uidlogNoChannel_df_temp = spark.sql(\n",
    "                '''\n",
    "                    SELECT \n",
    "                        acid,\n",
    "                        gid,\n",
    "                        sid,\n",
    "                        date,\n",
    "                        lt\n",
    "                    FROM acidnochanneldf\n",
    "                '''\n",
    "            )\n",
    "            \n",
    "            union_df = df_temp_uid_temp.union(uidlogNoChannel_df_temp)\n",
    "            \n",
    "            \n",
    "            final_acid_Result = union_df.rdd.map(lambda x: (x.acid, x)).reduceByKey(min_time).values()\n",
    "            final_acid_Result_fream = spark.createDataFrame(final_acid_Result)\n",
    "            final_acid_Result_fream.createOrReplaceTempView('acidlog')\n",
    "\n",
    "            xx = final_acid_Result_fream.toPandas()\n",
    "            print \"acidListMerge\",xx\n",
    "            upload(xx, 'cn-north-1', \"prodlog\", '/%s/acid_creatTime.json' % target_path)\n",
    "        else:\n",
    "            acidlogNoChannel_df.createOrReplaceTempView('acidlog')\n",
    "            upload(acidlogNoChannel_df.toPandas(), 'cn-north-1', \"prodlog\", '/%s/acid_creatTime.json' % target_path)\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"union today acid_creatTime \"\n",
    "\n",
    "        #渠道\n",
    "        ChannelList = getTempDataList(\"uid_ByChannelcreatTime\")\n",
    "        if len(ChannelList) > 0:\n",
    "            df_temp_uidChannel = spark.read.json(acidList)\n",
    "            \n",
    "            df_temp_uidChannel.createOrReplaceTempView('channelolddf')\n",
    "            \n",
    "            uidlogByChannel_df.createOrReplaceTempView('channelnochanneldf')\n",
    "            df_temp_uid_temp = spark.sql(\n",
    "                '''\n",
    "                    SELECT \n",
    "                        uid,\n",
    "                        gid,\n",
    "                        sid,\n",
    "                        date,\n",
    "                        lt,\n",
    "                        ch,\n",
    "                        machine\n",
    "                    FROM channelolddf\n",
    "                '''\n",
    "            )\n",
    "            \n",
    "            uidlogNoChannel_df_temp = spark.sql(\n",
    "                '''\n",
    "                    SELECT \n",
    "                        uid,\n",
    "                        gid,\n",
    "                        sid,\n",
    "                        date,\n",
    "                        lt,\n",
    "                        ch,\n",
    "                        machine\n",
    "                    FROM channelnochanneldf\n",
    "                '''\n",
    "            )\n",
    "            \n",
    "            union_df = df_temp_uid_temp.union(uidlogNoChannel_df_temp)\n",
    "            \n",
    "            \n",
    "            \n",
    "            final_uidChannel_Result = union_df.rdd.map(lambda x: (x.acid, x)).reduceByKey(\n",
    "                min_time).values()\n",
    "            final_uidChannel_Result_fream = spark.createDataFrame(final_uidChannel_Result)\n",
    "            final_uidChannel_Result_fream.createOrReplaceTempView('uidChannelLog')\n",
    "\n",
    "            xx = final_uidChannel_Result_fream.toPandas()\n",
    "            print \"channelListMerge\",xx\n",
    "            upload(xx, 'cn-north-1', \"prodlog\", '/%s/uid_ByChannelcreatTime.json' % target_path)\n",
    "        else:\n",
    "            uidlogByChannel_df.createOrReplaceTempView('uidChannelLog')\n",
    "            upload(uidlogByChannel_df.toPandas(), 'cn-north-1', \"prodlog\", '/%s/uidChannelLog.json' % target_path)\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"union today uid_ByChannelcreatTime \"\n",
    "\n",
    "        # 处理支付log\n",
    "        paylog_data_pd = paylog_data.toPandas()\n",
    "\n",
    "        # 读取支付历史数据\n",
    "        payList = getTempDataList(\"payLog\")\n",
    "        if len(payList) > 0:\n",
    "            historyPayLog = spark.read.json(payList)\n",
    "            \n",
    "            historyPayLog.createOrReplaceTempView('payolddf')\n",
    "            \n",
    "            paylog_data.createOrReplaceTempView('paytodaydf')\n",
    "            df_temp_uid_temp = spark.sql(\n",
    "                '''\n",
    "                    SELECT \n",
    "                        uid,\n",
    "                        acid,\n",
    "                        gid,\n",
    "                        sid,\n",
    "                        date,\n",
    "                        corpLvl,\n",
    "                        ch,\n",
    "                        vipLvl,\n",
    "                        payNum,\n",
    "                        payTime,\n",
    "                        LogTime\n",
    "                    FROM payolddf\n",
    "                '''\n",
    "            )\n",
    "            \n",
    "            uidlogNoChannel_df_temp = spark.sql(\n",
    "                '''\n",
    "                    SELECT \n",
    "                        uid,\n",
    "                        acid,\n",
    "                        gid,\n",
    "                        sid,\n",
    "                        date,\n",
    "                        corpLvl,\n",
    "                        ch,\n",
    "                        vipLvl,\n",
    "                        payNum,\n",
    "                        payTime,\n",
    "                        LogTime\n",
    "                    FROM paytodaydf\n",
    "                '''\n",
    "            )\n",
    "            \n",
    "            #union_df = df_temp_uid_temp.union(uidlogNoChannel_df_temp)\n",
    "            \n",
    "            result_pdf = df_temp_uid_temp.union(uidlogNoChannel_df_temp)\n",
    "            # 历史数据建立临时表\n",
    "            result_pdf.createOrReplaceTempView('payLog')\n",
    "            #上传支付log\n",
    "            upload(paylog_data_pd, 'cn-north-1', \"prodlog\", '/%s_payLog.json' % (targetpay_path +day))\n",
    "        else:\n",
    "            paylog_data.createOrReplaceTempView('payLog')\n",
    "            upload(paylog_data_pd, 'cn-north-1', \"prodlog\", '/%s_payLog.json' % (targetpay_path +day))\n",
    "\n",
    "\n",
    "        spark.sql(u'''\n",
    "                SELECT\n",
    "                    uid AS uid,\n",
    "                    SUM(payNum) AS sumpay,\n",
    "                    gid AS gid\n",
    "                FROM payLog\n",
    "                GROUP BY gid,uid\n",
    "                ''').createOrReplaceTempView('payByGid')\n",
    "\n",
    "        spark.sql(u'''\n",
    "                SELECT\n",
    "                    acid AS acid,\n",
    "                    SUM(payNum) AS sumpay,\n",
    "                    sid AS sid\n",
    "                FROM payLog\n",
    "                GROUP BY sid,acid\n",
    "                ''').createOrReplaceTempView('payBySid')\n",
    "\n",
    "        spark.sql(u'''\n",
    "                SELECT\n",
    "                    uid AS uid,\n",
    "                    SUM(payNum) AS sumpay,\n",
    "                    ch AS ch\n",
    "                FROM payLog\n",
    "                GROUP BY ch,uid\n",
    "                ''').createOrReplaceTempView('payByCh')\n",
    "        \n",
    "        #计算paylog的付费人数，付费金额，付费次数，\n",
    "        spark.sql(u'''\n",
    "                SELECT\n",
    "                    SUM(payNum) AS sumpay,\n",
    "                    gid AS gid,\n",
    "                    COUNT(DISTINCT uid) AS peoples,\n",
    "                    COUNT(s.uid) AS PayCounts\n",
    "                    \n",
    "                FROM payLog\n",
    "                GROUP BY gid\n",
    "                ''').createOrReplaceTempView('paySummaryByGid')\n",
    "\n",
    "        spark.sql(u'''\n",
    "                SELECT\n",
    "                    SUM(payNum) AS sumpay,\n",
    "                    sid AS sid,\n",
    "                    COUNT(DISTINCT uid) AS peoples,\n",
    "                    COUNT(s.uid) AS PayCounts\n",
    "                    \n",
    "                FROM payLog\n",
    "                GROUP BY sid\n",
    "                ''').createOrReplaceTempView('paySummaryBySid')\n",
    "\n",
    "        spark.sql(u'''\n",
    "                SELECT\n",
    "                    SUM(payNum) AS sumpay,\n",
    "                    ch AS ch,\n",
    "                    COUNT(DISTINCT uid) AS peoples,\n",
    "                    COUNT(s.uid) AS PayCounts\n",
    "                    \n",
    "                FROM payLog\n",
    "                GROUP BY ch\n",
    "                ''').createOrReplaceTempView('paySummaryByCh')\n",
    "        \n",
    "        \n",
    "        # 查出账号创建时间\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"start ltv\"\n",
    "        cToday = datetime.date(int(pp[0]), int(pp[1]), int(pp[2]))\n",
    "        \n",
    "        spark.sql(u'''\n",
    "                SELECT\n",
    "                    gid AS gid,\n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        )\n",
    "        \n",
    "        for i in range((cToday - firstDay).days + 1):\n",
    "            creatDay = u''' \"%s\"''' % str(firstDay + datetime.timedelta(days=i))\n",
    "            creatDayDateTime = firstDay + datetime.timedelta(days=i)\n",
    "            theday = (cToday - creatDayDateTime).days + 1\n",
    "            payResultByGid = spark.sql(u'''\n",
    "                                            SELECT\n",
    "                                                s.gid AS gid,\n",
    "                                                SUM(t.pay) AS payNum,\n",
    "                                                COUNT(s.uid) AS people,\n",
    "                                                first(s.date) AS date\n",
    "                                            FROM(\n",
    "                                            (SELECT\n",
    "                                                uid AS uid,\n",
    "                                                date AS date ,\n",
    "                                                gid AS gid\n",
    "                                            FROM uidlog\n",
    "                                            WHERE date = %s) s\n",
    "                                            LEFT OUTER JOIN(\n",
    "                                            SELECT\n",
    "                                                uid AS uid,\n",
    "                                                sumpay AS pay,\n",
    "                                                gid AS gid\n",
    "                                            FROM payByGid\n",
    "                                            ) t\n",
    "                                            on t.uid = s.uid)\n",
    "                                            GROUP BY s.gid\n",
    "                                        ''' % creatDay)\n",
    "\n",
    "            finalLTV_ByGid = payResultByGid.withColumn(\"days\", payResultByGid.people - payResultByGid.people + theday)\n",
    "            payResultBySid = spark.sql(u'''\n",
    "                                            SELECT\n",
    "                                                s.sid As sid,\n",
    "                                                SUM(t.payNum) AS payNum,\n",
    "                                                COUNT(s.acid) AS people,\n",
    "                                                first(s.date) AS date\n",
    "                                            FROM(\n",
    "                                            (SELECT\n",
    "                                                acid AS acid,\n",
    "                                                date AS date,\n",
    "                                                sid AS sid\n",
    "                                            FROM acidlog\n",
    "                                            WHERE date = %s) s\n",
    "                                            LEFT OUTER JOIN(\n",
    "                                            SELECT\n",
    "                                                acid AS acid,\n",
    "                                                sumpay AS payNum,\n",
    "                                                sid AS sid\n",
    "                                            FROM payBySid\n",
    "                                            ) t\n",
    "                                            on t.acid = s.acid)\n",
    "                                            GROUP BY s.sid\n",
    "                                        ''' % creatDay)\n",
    "\n",
    "            finalLTV_BySid = payResultBySid.withColumn(\"days\", payResultBySid.people - payResultBySid.people + theday)\n",
    "            payResultByCh = spark.sql(u'''\n",
    "                                            SELECT\n",
    "                                                s.ch AS ch,\n",
    "                                                SUM(t.pay) AS payNum,\n",
    "                                                COUNT(s.uid) AS people,\n",
    "                                                first(s.date)\n",
    "                                            FROM(\n",
    "                                            (SELECT\n",
    "                                                uid AS uid,\n",
    "                                                date AS date ,\n",
    "                                                ch AS ch\n",
    "                                            FROM uidChannelLog\n",
    "                                            WHERE date = %s) s\n",
    "                                            LEFT OUTER JOIN(\n",
    "                                            SELECT\n",
    "                                                uid AS uid,\n",
    "                                                sumpay AS pay,\n",
    "                                                ch AS ch\n",
    "                                            FROM payByCh\n",
    "                                            ) t\n",
    "                                            on t.uid = s.uid)\n",
    "                                            GROUP BY s.ch\n",
    "                                        ''' % creatDay)\n",
    "\n",
    "            finalLTV_ByCh = payResultByCh.withColumn(\"days\", payResultByCh.people - payResultByCh.people + theday)\n",
    "\n",
    "            payResultByGid_pd = finalLTV_ByGid.toPandas()\n",
    "            payResultBySid_pd = finalLTV_BySid.toPandas()\n",
    "            payResultByCh_pd = finalLTV_ByCh.toPandas()\n",
    "\n",
    "            # 替换 nan\n",
    "            payResultByGid_pd = payResultByGid_pd.where(payResultByGid_pd.notnull(), None)\n",
    "            payResultBySid_pd = payResultBySid_pd.where(payResultBySid_pd.notnull(), None)\n",
    "            payResultByCh_pd = payResultByCh_pd.where(payResultByCh_pd.notnull(), None)\n",
    "            print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"finish today ltv,begin ->sql\"\n",
    "            ltv_byGid_sqlalchemy_orm(payResultByGid_pd)\n",
    "            ltv_bySid_sqlalchemy_orm(payResultBySid_pd)\n",
    "            ltv_byCh_sqlalchemy_orm(payResultByCh_pd)\n",
    "        print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))), \"finish today data\"\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}